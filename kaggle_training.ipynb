{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b15fe4eb",
   "metadata": {},
   "source": [
    "## Step 1: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5818bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Clone the ACT-modification repository\n",
    "cd /kaggle/working\n",
    "git clone https://github.com/aryannzzz/ACT-modification.git\n",
    "cd ACT-modification\n",
    "echo \"Repository cloned successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f7ced6",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc6e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working/ACT-modification\n",
    "\n",
    "# Install the package in editable mode\n",
    "pip install -e .\n",
    "\n",
    "# Install additional dependencies for MetaWorld\n",
    "pip install metaworld gymnasium mujoco\n",
    "\n",
    "echo \"Dependencies installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d0058e",
   "metadata": {},
   "source": [
    "## Step 3: Verify GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88f9c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d158d98d",
   "metadata": {},
   "source": [
    "## Step 4: Configure WandB\n",
    "\n",
    "Login to Weights & Biases using Kaggle Secrets for experiment tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65620c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB Configuration - Login using Kaggle Secrets\n",
    "import wandb\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    secrets = UserSecretsClient()\n",
    "    wandb_key = secrets.get_secret(\"WANDB_API_KEY\")\n",
    "\n",
    "    if wandb_key:\n",
    "        wandb.login(key=wandb_key)\n",
    "        print(\"✅ Logged into Weights & Biases\")\n",
    "    else:\n",
    "        print(\"⚠️ WANDB_API_KEY is empty\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"⚠️ WANDB_API_KEY not found:\", e)\n",
    "    print(\"To use WandB: Add WANDB_API_KEY in Kaggle Secrets (Add-ons → Secrets)\")\n",
    "    # Disable WandB if key not found\n",
    "    os.environ['WANDB_MODE'] = 'disabled'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086132d4",
   "metadata": {},
   "source": [
    "## Step 5: Start Training\n",
    "\n",
    "This will train the ACT policy with the following configuration:\n",
    "- 50,000 training steps\n",
    "- Batch size: 8\n",
    "- Learning rate: 1e-4\n",
    "- Image augmentation enabled\n",
    "- Pretrained ResNet18 backbone\n",
    "- Chunk size: 50 action steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73755fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working/ACT-modification\n",
    "\n",
    "python src/lerobot/scripts/lerobot_train.py \\\n",
    "  --policy.type=act \\\n",
    "  --policy.repo_id=modified-act-reach-v3 \\\n",
    "  --policy.use_vae=true \\\n",
    "  --policy.vision_backbone=resnet18 \\\n",
    "  --policy.chunk_size=50 \\\n",
    "  --policy.n_action_steps=50 \\\n",
    "  --policy.n_obs_steps=1 \\\n",
    "  --policy.latent_dim=32 \\\n",
    "  --policy.kl_weight=10.0 \\\n",
    "  --policy.dim_model=256 \\\n",
    "  --policy.n_encoder_layers=4 \\\n",
    "  --policy.n_decoder_layers=1 \\\n",
    "  --policy.n_vae_encoder_layers=4 \\\n",
    "  --policy.pretrained_backbone_weights=ResNet18_Weights.IMAGENET1K_V1 \\\n",
    "  --env.type=metaworld \\\n",
    "  --env.task=reach-v3 \\\n",
    "  --dataset.repo_id=aadarshram/metaworld-reach-v3 \\\n",
    "  --dataset.image_transforms.enable=true \\\n",
    "  --steps=50000 \\\n",
    "  --batch_size=8 \\\n",
    "  --optimizer.lr=1e-4 \\\n",
    "  --eval_freq=-1 \\\n",
    "  --save_freq=25000 \\\n",
    "  --log_freq=250 \\\n",
    "  --wandb.enable=true \\\n",
    "  --wandb.project=metaworld-modified-act-reach \\\n",
    "  --wandb.disable_artifact=false \\\n",
    "  --output_dir=outputs/train/modified_act_reach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e4c48",
   "metadata": {},
   "source": [
    "## Step 6: Download Trained Model\n",
    "\n",
    "After training completes, download the checkpoints to use locally or for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e16b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Create a zip file of the trained model\n",
    "cd /kaggle/working/ACT-modification/outputs/train\n",
    "zip -r /kaggle/working/modified_act_reach_checkpoints.zip modified_act_reach/\n",
    "echo \"Checkpoints zipped! Download from /kaggle/working/modified_act_reach_checkpoints.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a355120",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate the Model (Optional)\n",
    "\n",
    "You can evaluate the trained model on 50 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b31b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working/ACT-modification\n",
    "\n",
    "python src/lerobot/scripts/lerobot_eval.py \\\n",
    "  --policy.path=outputs/train/modified_act_reach/checkpoints/050000/pretrained_model \\\n",
    "  --env.type=metaworld \\\n",
    "  --env.task=reach-v3 \\\n",
    "  --eval.batch_size=1 \\\n",
    "  --eval.n_episodes=50 \\\n",
    "  --policy.device=cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90badbe0",
   "metadata": {},
   "source": [
    "## Training Details\n",
    "\n",
    "**Model Architecture:**\n",
    "- Policy: ACT (Action Chunking Transformer) with VAE\n",
    "- Vision Backbone: ResNet18 (pretrained on ImageNet)\n",
    "- Transformer: 4 encoder layers, 1 decoder layer\n",
    "- Latent dimension: 32\n",
    "- Model dimension: 256\n",
    "\n",
    "**Training Configuration:**\n",
    "- Total steps: 50,000\n",
    "- Batch size: 8\n",
    "- Learning rate: 1e-4\n",
    "- Image augmentation: Enabled (ColorJitter, RandomAffine, SharpnessJitter)\n",
    "- Checkpoints saved every 25,000 steps\n",
    "\n",
    "**Dataset:**\n",
    "- Task: MetaWorld reach-v3\n",
    "- Episodes: 50\n",
    "- Frames: 2,370\n",
    "- Repo: aadarshram/metaworld-reach-v3\n",
    "\n",
    "**Expected Results:**\n",
    "- Previous baseline: 26% success rate (10k steps)\n",
    "- Target: Improved success rate with 5x more training"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
